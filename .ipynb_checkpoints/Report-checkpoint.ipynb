{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This report will describe the following in detail\n",
    "\n",
    "1. Learning Algorithm\n",
    "\n",
    "2. Plot of Rewards\n",
    "\n",
    "3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learning Algo\n",
    "\n",
    "The learning algorithm is the DQN with Experience Reply, as described in the paper \"Human-level control through deep reinforcement learning\", Althorithm 1: deep Q-learning with experience replay.\n",
    "\n",
    "The pseudo code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialize replay memory D to capacity N\n",
    "Initialize action value function Q with random weights\n",
    "Initialize target action-value function Q with weights equal to those of local Q\n",
    "\n",
    "\n",
    "for i in range(n_episodes): \n",
    "    reset the environment\n",
    "    get the first state\n",
    "    \n",
    "    for j in range(max_t):\n",
    "        agent choose an action, base on the current state and epsilon\n",
    "        base on this action, environment produces feedback, which includes next_state, reward, done\n",
    "        using the environment feedback, agend does the following:\n",
    "            1. add all the environment feedback to memory\n",
    "            2. if enough sample are available in memory, then get a random subset and learn\n",
    "            3. learning means updating the target Q, compare to the local Q, do gradient descent to minimize the error\n",
    "\n",
    "        update score and state\n",
    "        for every x steps, reset target Q to local Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a closer look at the stepthrough of the code, specifically the portion that describes how the agent act(chooses an action), and step (take the action, and observe the environment)\n",
    "\n",
    "- eps: the probability that the agent will select a random action over a learned one. will decay, eventually to a min value, as the episode continues.\n",
    "- gamma: it is the discount factor for rewards. goal for the agent is to maximize reward for the episode. however, immediate rewards matters more than future rewards\n",
    "- tau: while updating the local and target models, tau is an interpolation parameter used\n",
    "\n",
    "1. the intialized agent knows the state size, actions, have two Q networks (one is local, one is target), and memory (stores experience tuples)\n",
    "2. as the agent chooses an action, it considers the eps value. eps% of the time, the agent will select an random action. otherwise, it will take the action that generates the max value according to the current state\n",
    "3. as the agent steps, the agent's memory adds the experience tuple. when it is time to learn, agent samples an experience from the memory, and learn using that experience and gamma\n",
    "4. as the agent learns, the agent:\n",
    "    - get the max predicted Q value for the next state, \n",
    "    - calculate Q target for the current state (using gamma, rewards, Q value for next state)\n",
    "    - get the expected Q value from the local model\n",
    "    - compute loss by comparing Q target and Q expected\n",
    "    - miminize the loss by doing gradient decent\n",
    "    - update the target and local networks with tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot of Rewards\n",
    "\n",
    "![title](navigation_scores_v_episodes.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the agent achieved the goal of 13 at around 620 episodes. The scores reached a plateau at around 900 episodes, and yo-yo-ed back and forth at around 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ideas for Future Work\n",
    "\n",
    "Idea 1: \n",
    "We can tune the model setup and hyperparameters, as well as the agent parameters to see if we can achieve better performance sooner, or achieve higher scores. my current running average maxed out at around 16.\n",
    "\n",
    "Idea 2:\n",
    "We can also tune the models, as well as the agent parameters, or even try different methodologies to see if we can avoid having scores vary drastically from one episode to the next. The scores currently seem to have a very large variance to the mean. This makes it hard to determine which point I should stop training and to save the model.\n",
    "\n",
    "Idea 3: \n",
    "We can try different games, to see if we can generalize game types with agents using different algorithms parameters, and networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
